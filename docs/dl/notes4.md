# 《深度学习》笔记（4）

> 看《深度学习》记录的笔记，强烈建议广大深度学习从业者或者爱好者把这本书完整看一遍！

## 第四章 数值计算

### 4.1 上溢和下溢 

必须对上溢和下溢进行数值稳定的一个例子是 $ softmax$ 函数 ，通过计算 $ softmax(z)$  同时解决，其中 $ z = x - max_i x_i$ 。 

### 4.2 病态条件

条件数表征函数相对于输入的微小变化而变化的快慢程度。 输入被轻微扰动而迅速改变的函数对于科学计算来说可能是有问题的，因为输入中的舍入误差可能导致输出的巨大变化。 

### 4.3 基于梯度的优化方法 

优化指的是改变$ x $以最小化或最大化某个函数$ f(x) $的任务。 

导数对于最小化一个函数很有用，它告诉我们如何更改 $x$ 来略微地改善 $ y $ 。例如，我们知道对于足够小的 $ ϵ $ 来说，$ f(x - ϵsign(f ′(x))) $是比 $ f(x) $ 小的。因此我们可以将 $x$ 往导数的反方向移动一小步来减小 $ f(x) $。这种技术被称为 梯度下降（gradient descent） (Cauchy, 1847)。 

函数可能只有一个全局最小点或存在多个全局最小点，还可能存在不是全局最优的局部极小点。 

#### 4.3.1 梯度之上： Jacobian 和 Hessian 矩阵 

有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的偏导数的矩阵被称为**Jacobian**矩阵。 

二阶导数告诉我们，一阶导数将如何随着输入的变化而改变。它表示只基于梯度信息的梯度下降步骤是否会产生如我们预期的那样大的改善，因此它是重要的。我们可以认为， 二阶导数是对曲率的衡量。 

当我们的函数具有多维输入时， 二阶导数也有很多。我们可以将这些导数合并成一个矩阵，称为 **Hessian** 矩阵。 

最成功的特定优化领域或许是 凸优化（Convex optimization）。 凸优化通过更强的限制提供更多的保证。 凸优化算法只对凸函数适用，即 Hessian 处处半正定的函数。 

### 4.4 约束优化 

有时候，在 x 的所有可能值下最大化或最小化一个函数 f(x) 不是我们所希望的。相反，我们可能希望在 x 的某些集合 S 中找 f(x) 的最大值或最小值。这被称为 约束优化（constrained optimization）。 

约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修改。 

一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转化成原始约束优化问题的解。 

$ Karush–Kuhn–Tucker(KKT)$方法是针对约束优化非常通用的解决方案。 

广义 Lagrangian（generalized Lagrangian）或 广义 Lagrange 函数（generalized Lagrange function）:

$ L(x; λ; α) = f(x) + ∑_iλ_ig^{(i)}(x) + ∑_jα_jh^{(j)}(x)$

现在，我们可以通过优化无约束的广义 Lagrangian 解决约束最小化问题。 

等式约束对应项的符号并不重要；因为优化可以自由选择每个 λi 的符号，我们可以随意将其定义为加法或减法。 

我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质称为 Karush–Kuhn–Tucker（KKT）条件 (Karush, 1939; Kuhn and Tucker, 1951)。这些是确定一个点是最优点的必要条件，但不一定是充分条件。这些条件是：

* 广义 Lagrangian 的梯度为零。
* 所有关于 $x$ 和 KKT 乘子的约束都满足。
* 不等式约束显示的 ‘‘互补松弛性’’：$ α ⊙ h(x) = 0 $

### 4.5 实例：线性最小二乘 

略