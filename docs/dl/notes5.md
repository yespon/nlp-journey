# 第五章 机器学习基础 

大部分深度学习算法都是基于被称为随机梯度下降的算法求解的。 

## 5.1 学习算法 

对于某类任务 T 和性能度量P，一个计算机程序被认为可以从经验 E 中学习是指，通过经验 E 改进后，它在任务 T 上由性能度量 P 衡量的性能有所提升。 

### 5.1.1 任务 T 

一些非常常见的机器学习任务列举如下： 分类 、输入缺失分类 、回归 、转录、机器翻译 、结构化输出 、异常检测 、合成和采样 、缺失值填补 、去噪 、密度估计或概率质量函数估计

### 5.1.2 性能度量 P 

我们使用 测试集（test set）数据来评估系统性能，将其与训练机器学习系统的训练集数据分开。 

### 5.1.3 经验 E 

机器学习算法可以大致分类为 无监督（unsupervised）算法和 监督（supervised）算法。 

半监督学习中，一些样本有监督目标，但其他样本没有。 

强化学习（reinforcement learning）算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。 

表示数据集的常用方法是 设计矩阵（design matrix）。 设计矩阵的每一行包含一个不同的样本。每一列对应不同的特征。 

向量的维度不同的情况下，我们不会将数据集表示成 m 行的矩阵，而是表示成 m 个元素的结合:$ x^{(1)}, x^{(2)},  ..., x^{(m)}$。这种表示方式意味着样本向量$ x^{(i)} $和$ x^{(j)} $可以有不同的大小。 

### 5.1.4 示例：线性回归 

定义任务 T：通过输出 $y = w^⊤x$从 x 预测 y。 

度量模型性能的一种方法是计算模型在测试集上的 均方误差（mean squared error）。 

为了构建一个机器学习算法，我们需要设计一个算法，通过观察训练集$(X^{(train)}, y^{(train)}) $获得经验，减少 $MSE_{test}$ 以改进权重 w。 

## 5.2 容量、过拟合和欠拟合 

在先前未观测到的输入上表现良好的能力被称为 泛化（generalization） 

决定机器学习算法效果是否好的因素：
1. 降低训练误差。
2. 缩小训练误差和测试误差的差距。 

这两个因素对应机器学习的两个主要挑战： 欠拟合（underfitting）和 过拟合（overfitting）。  欠拟合是指模型不能在训练集上获得足够低的误差。而过拟合是指训练误差和和测试误差之间的差距太大。 

通过调整模型的 容量（capacity），可以控制模型是否偏向于过拟合或者欠拟合。通俗地，模型的容量是指其拟合各种函数的能力。

模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择 函数。这被称为模型的 表示容量（representational capacity）。 

实际中，学习算法不会真的找到最优函数，而仅是找到一个可以大大降低训练误差的函数。额外的限制因素，比如
优化算法的不完美，意味着学习算法的 有效容量（effective capacity）可能小于模型族的表示容量。 

更简单的函数更可能泛化（训练误差和测试误差的差距小），但我们仍然需要选择一个充分复杂的假设以达到低的训练误差。 

### 5.2.1 没有免费午餐定理 

在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。 

### 5.2.2 正则化 

在权重衰减的示例中，通过在最小化的目标中额外增加一项，明确地表示了偏好权重较小的线性函数。 

正则化（regularization）是指修改学习算法，使其降低泛化误差而非训练误差。

深度学习中普遍的理念是大量任务也许都可以使用非常通用的正则化形式来有效解决。 

## 5.3 超参数和验证集 

超参数的值不是通过学习算法本身学习出来的。

### 5.3.1 交叉验证 

k-折交叉验证算法可以用于估计学习算法 A 的泛化误差。 

## 5.4 估计、偏差和方差 

### 5.4.1 点估计 

虽然几乎所有的函数都可以称为估计量，但是一个良好的估计量的输出会接近生成训练数据的真实参数 θ。 

### 5.4.2 偏差 

